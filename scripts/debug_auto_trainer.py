#!/usr/bin/env python3
"""
Script de diagn√≥stico para problemas del auto-entrenamiento
"""

import subprocess
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_models_one_epoch():
    """Prueba cada modelo con 1 √©poca para identificar cu√°les fallan."""
    
    print("\nüéØ Probando modelos individuales con 1 √©poca...")
    
    # Modelos a probar (los que estaban fallando)
    models_to_test = [
        'resnet50',
        'resnet101', 
        'efficientnet_b0',
        'efficientnet_b1',
        'efficientnet_b2',
        'efficientnet_b3',
        'efficientnet_b4',
        'convnext_tiny'
    ]
    
    results = {}
    
    for model in models_to_test:
        print(f"\nüîç Probando {model}...")
        
        # Comando b√°sico de 1 √©poca
        cmd = [
            sys.executable, 'scripts/train_model.py',
            '--model', model,
            '--epochs', '1',
            '--batch-size', '8',  # Batch size peque√±o para evitar OOM
            '--learning-rate', '0.001',
            '--experiment-name', f'debug_{model}',
            '--quick-test'  # Sin dry-run, que entrene de verdad
        ]
        
        print(f"   Comando: {' '.join(cmd[-6:])}")  # Solo los argumentos relevantes
        
        try:
            print(f"   üöÄ Ejecutando: {' '.join(cmd[-6:])}")  # Solo los argumentos relevantes
            
            # Ejecutar CON logs visibles en tiempo real
            result = subprocess.run(
                cmd,
                cwd=project_root,
                capture_output=False,  # ¬°AQU√ç! Mostrar logs en tiempo real
                text=True,
                timeout=300  # 5 minutos m√°ximo por modelo
            )
            
            print(f"\n   üìä Resultado del proceso: Return code = {result.returncode}")
            
            if result.returncode == 0:
                print(f"   ‚úÖ {model} FUNCIONA")
                results[model] = 'OK'
            else:
                print(f"   ‚ùå {model} FALL√ì (return code: {result.returncode})")
                results[model] = 'FAIL'
                
        except subprocess.TimeoutExpired:
            print(f"   ‚è∞ {model} TIMEOUT")
            results[model] = 'TIMEOUT'
        
        except Exception as e:
            print(f"   üí• {model} ERROR: {e}")
            results[model] = f'ERROR: {e}'
    
    return results

def test_specific_arguments():
    """Prueba argumentos espec√≠ficos que pueden estar causando problemas."""
    
    print("\nüîß Probando argumentos espec√≠ficos...")
    
    # Argumentos a probar
    args_to_test = [
        # B√°sico
        [],
        # Focal loss
        ['--focal-loss'],
        # Aggressive aug
        ['--aggressive-aug'],
        # Class weights
        ['--class-weights'],
        # Mixed precision
        ['--mixed-precision'],
        # Combination
        ['--focal-loss', '--class-weights'],
        # Samplers (los que estaban fallando)
        # ['--sampler', 'site_aware'],
        # ['--sampler', 'weighted'],
        # ['--sampler', 'balanced_batch']
    ]
    
    results = {}
    
    for i, extra_args in enumerate(args_to_test):
        test_name = f"args_test_{i}_{('_'.join(extra_args)).replace('--', '')}"
        print(f"\nüîç Probando argumentos: {' '.join(extra_args) if extra_args else '(b√°sico)'}")
        
        cmd = [
            sys.executable, 'scripts/train_model.py',
            '--model', 'resnet50',  # Modelo que sabemos que funciona
            '--epochs', '1',
            '--batch-size', '8',
            '--learning-rate', '0.001',
            '--experiment-name', test_name,
            '--quick-test'
        ] + extra_args
        
        try:
            print(f"   üöÄ Ejecutando argumentos: {' '.join(extra_args) if extra_args else '(b√°sico)'}")
            
            # Ejecutar CON logs visibles en tiempo real
            result = subprocess.run(
                cmd,
                cwd=project_root,
                capture_output=False,  # ¬°AQU√ç! Mostrar logs en tiempo real
                text=True,
                timeout=180  # 3 minutos
            )
            
            print(f"\n   üìä Resultado del proceso: Return code = {result.returncode}")
            
            if result.returncode == 0:
                print(f"   ‚úÖ Argumentos FUNCIONAN")
                results[test_name] = 'OK'
            else:
                print(f"   ‚ùå Argumentos FALLAN")
                results[test_name] = 'FAIL'
        
        except Exception as e:
            print(f"   üí• ERROR: {e}")
            results[test_name] = f'ERROR: {e}'
    
    return results

def test_imports():
    """Prueba las importaciones b√°sicas."""
    
    print("\nüîç Verificando importaciones...")
    
    try:
        from src.utils.config import Config
        print("‚úÖ Config import OK")
        
        from src.data import DataLoaderManager
        print("‚úÖ DataLoaderManager import OK")
        
        from src.models.model import create_model
        print("‚úÖ create_model import OK")
        
        # Test config loading
        config = Config("configs/base_config.yaml")
        print("‚úÖ Config loading OK")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Import error: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_data_availability():
    """Verifica que los datos est√©n disponibles."""
    
    print("\nüîç Verificando datos...")
    
    data_dir = project_root / "data" / "raw"
    
    required_files = [
        "train_features.csv",
        "train_labels.csv",
        "test_features.csv"
    ]
    
    all_good = True
    
    for file in required_files:
        file_path = data_dir / file
        if file_path.exists():
            print(f"‚úÖ {file} existe")
        else:
            print(f"‚ùå {file} NO existe")
            all_good = False
    
    # Check validation sites
    val_sites = project_root / "data" / "processed" / "validation_sites.csv"
    if val_sites.exists():
        print(f"‚úÖ validation_sites.csv existe")
    else:
        print(f"‚ùå validation_sites.csv NO existe")
        all_good = False
    
    return all_good

def main():
    """Funci√≥n principal de diagn√≥stico."""
    
    print("üöÄ Iniciando diagn√≥stico COMPLETO del auto-entrenamiento")
    print("üîç NOTA: Ver√°s TODOS los logs de entrenamiento en tiempo real")
    print("=" * 60)
    
    # 1. Test imports
    imports_ok = test_imports()
    
    # 2. Check data
    data_ok = check_data_availability()
    
    if not all([imports_ok, data_ok]):
        print("\nüö® Problemas b√°sicos encontrados. Arreglar antes de continuar.")
        return
    
    # 3. Test cada modelo individual
    print(f"\n{'='*60}")
    print("üéØ FASE 1: PROBANDO MODELOS INDIVIDUALES (1 √©poca cada uno)")
    print("üïê Esto puede tardar 15-30 minutos total...")
    print("="*60)
    model_results = test_models_one_epoch()
    
    # 4. Test argumentos espec√≠ficos
    print(f"\n{'='*60}")
    print("üîß FASE 2: PROBANDO ARGUMENTOS ESPEC√çFICOS")
    print("üïê Esto tardar√° ~10-15 minutos m√°s...")
    print("="*60)
    args_results = test_specific_arguments()
    
    # 5. Resumen final
    print(f"\n{'='*60}")
    print("üìä RESUMEN COMPLETO DEL DIAGN√ìSTICO")
    print("="*60)
    
    print(f"Importaciones: {'‚úÖ OK' if imports_ok else '‚ùå FAIL'}")
    print(f"Datos: {'‚úÖ OK' if data_ok else '‚ùå FAIL'}")
    
    print(f"\nüß† RESULTADOS DE MODELOS:")
    working_models = []
    failing_models = []
    
    for model, status in model_results.items():
        status_icon = "‚úÖ" if status == "OK" else "‚ùå"
        print(f"   {status_icon} {model}: {status}")
        
        if status == "OK":
            working_models.append(model)
        else:
            failing_models.append(model)
    
    print(f"\nüîß RESULTADOS DE ARGUMENTOS:")
    working_args = []
    failing_args = []
    
    for test_name, status in args_results.items():
        status_icon = "‚úÖ" if status == "OK" else "‚ùå"
        args_display = test_name.replace('args_test_', '').replace('_', ' ')
        print(f"   {status_icon} {args_display}: {status}")
        
        if status == "OK":
            working_args.append(test_name)
        else:
            failing_args.append(test_name)
    
    # Recomendaciones
    print(f"\nüí° RECOMENDACIONES:")
    
    if working_models:
        print(f"‚úÖ Modelos que FUNCIONAN: {', '.join(working_models)}")
        print(f"   -> Usar estos modelos en el auto-entrenamiento")
    
    if failing_models:
        print(f"‚ùå Modelos que FALLAN: {', '.join(failing_models)}")
        print(f"   -> Evitar estos modelos o investigar errores espec√≠ficos")
    
    if working_args:
        print(f"‚úÖ Argumentos que FUNCIONAN: Usar combinaciones exitosas")
    
    if failing_args:
        print(f"‚ùå Argumentos problem√°ticos: Evitar o usar alternativas")
    
    print(f"\nüîÑ PR√ìXIMOS PASOS:")
    if working_models:
        print(f"1. Actualizar auto_train_multiple.py para usar solo modelos que funcionan")
        print(f"2. Usar argumentos que se probaron exitosamente")
        print(f"3. Ejecutar auto-entrenamiento con configuraci√≥n segura")
    else:
        print(f"1. Investigar errores espec√≠ficos de los modelos")
        print(f"2. Verificar instalaci√≥n de timm y dependencias")
        print(f"3. Probar con entorno virtual limpio")
    
    return {
        'models': model_results,
        'args': args_results,
        'working_models': working_models,
        'failing_models': failing_models
    }

if __name__ == "__main__":
    main()