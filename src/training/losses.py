"""
TaÃ¯ National Park - Custom Loss Functions

This module provides specialized loss functions for camera trap species classification.
Includes focal loss, label smoothing, and class-balanced losses.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tensor, Union, List


class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance.
    
    Focal Loss = -Î±(1-pt)^Î³ * log(pt)
    
    Reference:
    Lin, T. Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017).
    Focal loss for dense object detection.
    """
    
    def __init__(
        self, 
        alpha: Optional[Union[float, List[float], Tensor]] = None,
        gamma: float = 2.0,
        reduction: str = 'mean',
        ignore_index: int = -100
    ):
        """
        Initialize Focal Loss.
        
        Args:
            alpha: Weighting factor for classes (None, scalar, or tensor)
            gamma: Focusing parameter
            reduction: Reduction method ('mean', 'sum', 'none')
            ignore_index: Index to ignore in loss calculation
        """
        super(FocalLoss, self).__init__()
        
        if isinstance(alpha, (float, int)):
            self.alpha = torch.tensor([alpha, 1 - alpha])
        elif isinstance(alpha, list):
            self.alpha = torch.tensor(alpha)
        else:
            self.alpha = alpha
        
        self.gamma = gamma
        self.reduction = reduction
        self.ignore_index = ignore_index
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C] or [N, C, H, W]
            targets: Ground truth [N] or [N, H, W]
            
        Returns:
            Focal loss value
        """
        
        # Calculate cross entropy
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=self.ignore_index)
        
        # Calculate pt
        pt = torch.exp(-ce_loss)
        
        # Calculate alpha term
        if self.alpha is not None:
            if self.alpha.type() != inputs.data.type():
                self.alpha = self.alpha.type_as(inputs.data)
            
            alpha_t = self.alpha[targets]
            focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss
        else:
            focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingCrossEntropy(nn.Module):
    """
    Label Smoothing Cross Entropy Loss.
    
    Helps with overconfident predictions and improves generalization.
    """
    
    def __init__(self, smoothing: float = 0.1, reduction: str = 'mean'):
        """
        Initialize Label Smoothing Cross Entropy.
        
        Args:
            smoothing: Smoothing factor (0.0 = no smoothing)
            reduction: Reduction method ('mean', 'sum', 'none')
        """
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing
        self.reduction = reduction
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Label smoothing cross entropy loss
        """
        
        num_classes = inputs.size(1)
        log_probs = F.log_softmax(inputs, dim=1)
        
        # Create smoothed targets
        with torch.no_grad():
            true_dist = torch.zeros_like(log_probs)
            true_dist.fill_(self.smoothing / (num_classes - 1))
            true_dist.scatter_(1, targets.data.unsqueeze(1), 1.0 - self.smoothing)
        
        loss = -true_dist * log_probs
        
        if self.reduction == 'mean':
            return loss.sum(dim=1).mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss.sum(dim=1)


class ClassBalancedLoss(nn.Module):
    """
    Class-Balanced Loss based on effective number of samples.
    
    Reference:
    Cui, Y., Jia, M., Lin, T. Y., Song, Y., & Belongie, S. (2019).
    Class-balanced loss based on effective number of samples.
    """
    
    def __init__(
        self,
        samples_per_class: List[int],
        beta: float = 0.9999,
        gamma: float = 2.0,
        loss_type: str = 'focal'
    ):
        """
        Initialize Class-Balanced Loss.
        
        Args:
            samples_per_class: Number of samples per class
            beta: Hyperparameter for re-weighting
            gamma: Focusing parameter for focal loss
            loss_type: Type of loss ('focal', 'sigmoid', 'softmax')
        """
        super(ClassBalancedLoss, self).__init__()
        
        self.samples_per_class = samples_per_class
        self.beta = beta
        self.gamma = gamma
        self.loss_type = loss_type
        
        # Calculate effective number of samples
        effective_num = 1.0 - np.power(beta, samples_per_class)
        weights = (1.0 - beta) / np.array(effective_num)
        weights = weights / np.sum(weights) * len(weights)
        
        self.weights = torch.tensor(weights, dtype=torch.float32)
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Class-balanced loss value
        """
        
        if self.weights.device != inputs.device:
            self.weights = self.weights.to(inputs.device)
        
        if self.loss_type == 'focal':
            focal_loss = FocalLoss(alpha=self.weights, gamma=self.gamma)
            return focal_loss(inputs, targets)
        elif self.loss_type == 'softmax':
            return F.cross_entropy(inputs, targets, weight=self.weights)
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")


class AsymmetricLoss(nn.Module):
    """
    Asymmetric Loss for Multi-label Classification.
    
    Can be adapted for single-label classification with class imbalance.
    """
    
    def __init__(
        self,
        gamma_neg: float = 4.0,
        gamma_pos: float = 1.0,
        clip: float = 0.05,
        eps: float = 1e-8,
        disable_torch_grad_focal_loss: bool = False
    ):
        """
        Initialize Asymmetric Loss.
        
        Args:
            gamma_neg: Focusing parameter for negative samples
            gamma_pos: Focusing parameter for positive samples
            clip: Probability clipping threshold
            eps: Small epsilon for numerical stability
            disable_torch_grad_focal_loss: Whether to disable gradients for focal loss
        """
        super(AsymmetricLoss, self).__init__()
        
        self.gamma_neg = gamma_neg
        self.gamma_pos = gamma_pos
        self.clip = clip
        self.eps = eps
        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Asymmetric loss value
        """
        
        # Convert targets to one-hot
        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()
        
        # Apply sigmoid and clip
        xs_pos = torch.sigmoid(inputs)
        xs_neg = 1 - xs_pos
        
        # Clipping
        if self.clip is not None and self.clip > 0:
            xs_neg = (xs_neg + self.clip).clamp(max=1)
        
        # Calculate loss
        los_pos = targets_one_hot * torch.log(xs_pos.clamp(min=self.eps))
        los_neg = (1 - targets_one_hot) * torch.log(xs_neg.clamp(min=self.eps))
        
        # Asymmetric focusing
        if self.gamma_neg > 0 or self.gamma_pos > 0:
            if self.disable_torch_grad_focal_loss:
                torch.set_grad_enabled(False)
            
            pt0 = xs_pos * targets_one_hot
            pt1 = xs_neg * (1 - targets_one_hot)
            pt = pt0 + pt1
            one_sided_gamma = self.gamma_pos * targets_one_hot + self.gamma_neg * (1 - targets_one_hot)
            one_sided_w = torch.pow(1 - pt, one_sided_gamma)
            
            if self.disable_torch_grad_focal_loss:
                torch.set_grad_enabled(True)
            
            los_pos *= one_sided_w
            los_neg *= one_sided_w
        
        loss = los_pos + los_neg
        return -loss.sum(dim=1).mean()


class DiceLoss(nn.Module):
    """
    Dice Loss for handling class imbalance.
    
    Originally designed for segmentation but can be adapted for classification.
    """
    
    def __init__(self, smooth: float = 1.0, reduction: str = 'mean'):
        """
        Initialize Dice Loss.
        
        Args:
            smooth: Smoothing factor to avoid division by zero
            reduction: Reduction method ('mean', 'sum', 'none')
        """
        super(DiceLoss, self).__init__()
        self.smooth = smooth
        self.reduction = reduction
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Dice loss value
        """
        
        # Convert to probabilities
        probs = F.softmax(inputs, dim=1)
        
        # Convert targets to one-hot
        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()
        
        # Calculate Dice coefficient
        intersection = (probs * targets_one_hot).sum(dim=0)
        union = probs.sum(dim=0) + targets_one_hot.sum(dim=0)
        
        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice
        
        if self.reduction == 'mean':
            return dice_loss.mean()
        elif self.reduction == 'sum':
            return dice_loss.sum()
        else:
            return dice_loss


class CombinedLoss(nn.Module):
    """
    Combined loss function that combines multiple loss types.
    """
    
    def __init__(
        self,
        losses: List[nn.Module],
        weights: Optional[List[float]] = None,
        reduction: str = 'mean'
    ):
        """
        Initialize Combined Loss.
        
        Args:
            losses: List of loss functions
            weights: Weights for each loss function
            reduction: Reduction method
        """
        super(CombinedLoss, self).__init__()
        
        self.losses = nn.ModuleList(losses)
        
        if weights is None:
            self.weights = [1.0] * len(losses)
        else:
            self.weights = weights
        
        self.reduction = reduction
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Combined loss value
        """
        
        total_loss = 0.0
        
        for loss_fn, weight in zip(self.losses, self.weights):
            loss_value = loss_fn(inputs, targets)
            total_loss += weight * loss_value
        
        return total_loss


class OnlineHardExampleMining(nn.Module):
    """
    Online Hard Example Mining (OHEM) for focusing on hard examples.
    """
    
    def __init__(
        self,
        ratio: float = 0.7,
        min_kept: int = 1,
        base_loss: nn.Module = None
    ):
        """
        Initialize OHEM.
        
        Args:
            ratio: Ratio of samples to keep
            min_kept: Minimum number of samples to keep
            base_loss: Base loss function
        """
        super(OnlineHardExampleMining, self).__init__()
        
        self.ratio = ratio
        self.min_kept = min_kept
        self.base_loss = base_loss or nn.CrossEntropyLoss(reduction='none')
    
    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            inputs: Predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            OHEM loss value
        """
        
        # Calculate loss for all samples
        losses = self.base_loss(inputs, targets)
        
        # Sort losses and keep top ratio
        batch_size = inputs.size(0)
        num_kept = max(self.min_kept, int(batch_size * self.ratio))
        
        sorted_losses, _ = torch.sort(losses, descending=True)
        kept_losses = sorted_losses[:num_kept]
        
        return kept_losses.mean()


class DistillationLoss(nn.Module):
    """
    Knowledge Distillation Loss for model compression or ensemble training.
    """
    
    def __init__(
        self,
        temperature: float = 4.0,
        alpha: float = 0.5,
        base_loss: nn.Module = None
    ):
        """
        Initialize Distillation Loss.
        
        Args:
            temperature: Temperature for softmax
            alpha: Weight for distillation loss
            base_loss: Base loss function for hard targets
        """
        super(DistillationLoss, self).__init__()
        
        self.temperature = temperature
        self.alpha = alpha
        self.base_loss = base_loss or nn.CrossEntropyLoss()
    
    def forward(
        self, 
        student_logits: Tensor, 
        teacher_logits: Tensor, 
        targets: Tensor
    ) -> Tensor:
        """
        Forward pass.
        
        Args:
            student_logits: Student model predictions [N, C]
            teacher_logits: Teacher model predictions [N, C]
            targets: Ground truth [N]
            
        Returns:
            Distillation loss value
        """
        
        # Distillation loss
        student_probs = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
        
        distillation_loss = F.kl_div(
            student_probs, teacher_probs, reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # Hard target loss
        hard_loss = self.base_loss(student_logits, targets)
        
        # Combined loss
        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * hard_loss
        
        return total_loss


class SupConLoss(nn.Module):
    """
    Supervised Contrastive Learning Loss.
    
    Reference:
    Khosla, P., et al. (2020). Supervised contrastive learning.
    """
    
    def __init__(self, temperature: float = 0.07, contrast_mode: str = 'all'):
        """
        Initialize Supervised Contrastive Loss.
        
        Args:
            temperature: Temperature parameter
            contrast_mode: Contrast mode ('all' or 'one')
        """
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
    
    def forward(self, features: Tensor, labels: Tensor) -> Tensor:
        """
        Forward pass.
        
        Args:
            features: Feature representations [N, D]
            labels: Ground truth labels [N]
            
        Returns:
            Supervised contrastive loss value
        """
        
        batch_size = features.shape[0]
        device = features.device
        
        # Normalize features
        features = F.normalize(features, dim=1)
        
        # Create label mask
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)
        
        # Compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(features, features.T), self.temperature
        )
        
        # For numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        
        # Tile mask
        if self.contrast_mode == 'all':
            anchor_count = batch_size
        else:
            anchor_count = 1
        
        # Create mask for positive pairs
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask
        
        # Compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))
        
        # Compute mean of log-likelihood over positive
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)
        
        # Loss
        loss = -mean_log_prob_pos
        loss = loss.view(anchor_count, batch_size).mean()
        
        return loss


# Utility functions for loss selection
def get_loss_function(
    loss_name: str,
    class_weights: Optional[Tensor] = None,
    num_classes: int = 8,
    **kwargs
) -> nn.Module:
    """
    Factory function to create loss functions.
    
    Args:
        loss_name: Name of the loss function
        class_weights: Class weights for imbalanced datasets
        num_classes: Number of classes
        **kwargs: Additional arguments for loss function
        
    Returns:
        Loss function instance
    """
    
    loss_name = loss_name.lower()
    
    if loss_name == 'cross_entropy':
        return nn.CrossEntropyLoss(weight=class_weights, **kwargs)
    
    elif loss_name == 'focal':
        return FocalLoss(alpha=class_weights, **kwargs)
    
    elif loss_name == 'label_smoothing':
        return LabelSmoothingCrossEntropy(**kwargs)
    
    elif loss_name == 'class_balanced':
        if 'samples_per_class' not in kwargs:
            raise ValueError("samples_per_class required for class_balanced loss")
        return ClassBalancedLoss(**kwargs)
    
    elif loss_name == 'asymmetric':
        return AsymmetricLoss(**kwargs)
    
    elif loss_name == 'dice':
        return DiceLoss(**kwargs)
    
    elif loss_name == 'ohem':
        base_loss = nn.CrossEntropyLoss(weight=class_weights, reduction='none')
        return OnlineHardExampleMining(base_loss=base_loss, **kwargs)
    
    elif loss_name == 'combined':
        # Example: Combine CrossEntropy and Focal Loss
        losses = [
            nn.CrossEntropyLoss(weight=class_weights),
            FocalLoss(alpha=class_weights, gamma=2.0)
        ]
        weights = kwargs.get('weights', [0.5, 0.5])
        return CombinedLoss(losses=losses, weights=weights)
    
    else:
        raise ValueError(f"Unknown loss function: {loss_name}")


def compute_class_weights(
    class_counts: List[int],
    method: str = 'inverse_frequency',
    power: float = 1.0
) -> Tensor:
    """
    Compute class weights for imbalanced datasets.
    
    Args:
        class_counts: Number of samples per class
        method: Method to compute weights ('inverse_frequency', 'effective_number')
        power: Power to raise the weights to
        
    Returns:
        Class weights tensor
    """
    
    class_counts = np.array(class_counts)
    total_samples = class_counts.sum()
    num_classes = len(class_counts)
    
    if method == 'inverse_frequency':
        # Inverse frequency weighting
        weights = total_samples / (num_classes * class_counts)
    
    elif method == 'effective_number':
        # Effective number of samples
        beta = 0.9999
        effective_num = 1.0 - np.power(beta, class_counts)
        weights = (1.0 - beta) / effective_num
    
    elif method == 'balanced':
        # Balanced weighting
        weights = total_samples / (num_classes * class_counts)
        weights = weights / weights.sum() * num_classes
    
    else:
        raise ValueError(f"Unknown weighting method: {method}")
    
    # Apply power
    if power != 1.0:
        weights = np.power(weights, power)
    
    # Normalize
    weights = weights / weights.sum() * num_classes
    
    return torch.tensor(weights, dtype=torch.float32)


def adaptive_loss_weighting(
    losses: List[float],
    method: str = 'uncertainty',
    temperature: float = 1.0
) -> List[float]:
    """
    Compute adaptive weights for multiple losses.
    
    Args:
        losses: List of loss values
        method: Method for computing weights ('uncertainty', 'gradient_norm')
        temperature: Temperature for softmax weighting
        
    Returns:
        List of adaptive weights
    """
    
    losses = np.array(losses)
    
    if method == 'uncertainty':
        # Uncertainty-based weighting
        weights = 1.0 / (2 * losses)
        weights = weights / weights.sum() * len(losses)
    
    elif method == 'gradient_norm':
        # This would require gradient information
        # For now, use inverse loss as proxy
        weights = 1.0 / losses
        weights = weights / weights.sum() * len(losses)
    
    elif method == 'softmax':
        # Softmax weighting
        weights = np.exp(-losses / temperature)
        weights = weights / weights.sum() * len(losses)
    
    else:
        # Equal weighting
        weights = np.ones(len(losses)) / len(losses)
    
    return weights.tolist()


# Example usage and testing
if __name__ == "__main__":
    print("Testing loss functions...")
    
    # Test data
    batch_size = 32
    num_classes = 8
    
    # Dummy data
    inputs = torch.randn(batch_size, num_classes)
    targets = torch.randint(0, num_classes, (batch_size,))
    
    # Test basic losses
    losses_to_test = [
        ('CrossEntropy', nn.CrossEntropyLoss()),
        ('Focal', FocalLoss(gamma=2.0)),
        ('LabelSmoothing', LabelSmoothingCrossEntropy(smoothing=0.1)),
        ('Dice', DiceLoss()),
        ('OHEM', OnlineHardExampleMining(ratio=0.7))
    ]
    
    for name, loss_fn in losses_to_test:
        try:
            loss_value = loss_fn(inputs, targets)
            print(f"âœ… {name}: {loss_value.item():.4f}")
        except Exception as e:
            print(f"âŒ {name}: {e}")
    
    # Test class weights computation
    class_counts = [2474, 1641, 2213, 2423, 978, 2254, 2492, 2013]  # From your data
    weights = compute_class_weights(class_counts, method='inverse_frequency')
    print(f"âœ… Class weights: {weights}")
    
    # Test with class weights
    weighted_ce = nn.CrossEntropyLoss(weight=weights)
    loss_value = weighted_ce(inputs, targets)
    print(f"âœ… Weighted CrossEntropy: {loss_value.item():.4f}")
    
    print("\nğŸ‰ All loss function tests passed!")
    print("\nKey features implemented:")
    print("  ğŸ¯ Focal Loss for class imbalance")
    print("  ğŸ·ï¸  Label Smoothing for better generalization")
    print("  âš–ï¸  Class-balanced loss with effective number")
    print("  ğŸ”„ Online Hard Example Mining")
    print("  ğŸ“š Knowledge Distillation support")
    print("  ğŸ­ Multiple loss combination")
    print("  ğŸ§® Adaptive loss weighting")